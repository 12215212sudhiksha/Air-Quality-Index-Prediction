{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***AIR QUALITY INDEX PREDICTION USING ML***"
      ],
      "metadata": {
        "id": "6opPCYYzkmA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Infromation:**\n",
        "\n",
        "1. Date (DD/MM/YYYY)  \n",
        "2. Time (HH.MM.SS)  \n",
        "3. True hourly averaged concentration CO in mg/m^3 (reference analyzer)  \n",
        "4. PT08.S1 (tin oxide) hourly averaged sensor response (nominally CO targeted)  \n",
        "5. True hourly averaged overall Non Methanic HydroCarbons concentration in microg/m^3 (reference analyzer)  \n",
        "6. True hourly averaged Benzene concentration in microg/m^3 (reference analyzer)  \n",
        "7. PT08.S2 (titania) hourly averaged sensor response (nominally NMHC targeted)  \n",
        "8. True hourly averaged NOx concentration in ppb (reference analyzer)  \n",
        "9. PT08.S3 (tungsten oxide) hourly averaged sensor response (nominally NOx targeted)  \n",
        "10. True hourly averaged NO2 concentration in microg/m^3 (reference analyzer)  \n",
        "11. PT08.S4 (tungsten oxide) hourly averaged sensor response (nominally NO2 targeted)  \n",
        "12. PT08.S5 (indium oxide) hourly averaged sensor response (nominally O3 targeted)  \n",
        "13. Temperature in Â°C  \n",
        "14. Relative Humidity (%)  \n",
        "15. AH Absolute Humidity\n"
      ],
      "metadata": {
        "id": "uHwnjH-1liIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Set Information:\n",
        "The dataset contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device. The device was located on the field in a significantly polluted area, at road level, within an Italian city. **Data were recorded from March 2004 to February 2005 (one year)** representing the longest freely available recordings of on-field deployed air quality chemical sensor devices responses. Ground Truth hourly averaged concentrations for CO, Non Methanic Hydrocarbons, Benzene, Total Nitrogen Oxides (NOx) and Nitrogen Dioxide (NO2) and were provided by a co-located reference certified analyzer. Evidences of cross-sensitivities as well as both concept and sensor drifts are present as described in De Vito et al., Sens. And Act. B, Vol. 129, 2.2.2008 (citation required) eventually affecting sensors concentration estimation capabilities. **Missing values are tagged with -200 value**.\n",
        "This dataset can be used exclusively for research purposes. Commercial purposes are fully excluded."
      ],
      "metadata": {
        "id": "r6cUKFuYmJFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github repository: https://github.com/12215212sudhiksha/Air-Quality-Index-Prediction"
      ],
      "metadata": {
        "id": "6v0rLIhkk0IA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdAhru5qnPXQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Air_Quality.csv', sep =';', decimal = ',')\n",
        "#All the values in csv file are seperated by semicolon and few colums contains ',' instead of decimal"
      ],
      "metadata": {
        "id": "pd7E3bH-GFrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "0YrwOsltH9LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dropping the unwanted columns"
      ],
      "metadata": {
        "id": "1YUTlCghJDFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing the last 2 columns from the dataframe\n",
        "df = df.iloc[:, :-2]"
      ],
      "metadata": {
        "id": "0OneCl6yH5ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pp1W3oIwJX_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "P6_W90XQJaMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape\n"
      ],
      "metadata": {
        "id": "ZZ5W6b5uJh-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Remove NaN rows"
      ],
      "metadata": {
        "id": "SyylcRoXJ9uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "qiimBdJbP8bP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.isna(), yticklabels=False, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FsCrIADEQG0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[[9356]]"
      ],
      "metadata": {
        "id": "O8YMAg-PJp0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9356 represents the last data point in the dataframe and remaining rows are just null values."
      ],
      "metadata": {
        "id": "L_vUd-hgKwMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(9357)"
      ],
      "metadata": {
        "id": "q6ksr5WxKkrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "y-vA9NrmNQ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "Y9G0LLf5NUXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "v7x2tbf5QltL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "t-VaLaFmQ5QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Ta4aauhWQ_nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows thatbhere are no missing values in the dataset. But the actual missing values are tagged with the value \"-200\"."
      ],
      "metadata": {
        "id": "4kSKg6z-Reip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting the number of time -200 appears in the data\n",
        "df.isin([-200]).sum(axis=0)"
      ],
      "metadata": {
        "id": "SFi4NpmQRJ5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling the missing values"
      ],
      "metadata": {
        "id": "-VOa-5ENSF5v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert all -200 to NaN"
      ],
      "metadata": {
        "id": "lG3UAu9-TKf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace all NaN values with mean of that specific column."
      ],
      "metadata": {
        "id": "oDxOprTyTOXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.replace(to_replace=-200, value = np.nan)"
      ],
      "metadata": {
        "id": "MsbuRVAaSA9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "cFx803YWTs_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows the actual number of missing values"
      ],
      "metadata": {
        "id": "fdZuZl2UUJDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "BgTLe5ipUHLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select_dtypes(include='number').mean()\n"
      ],
      "metadata": {
        "id": "0fjxS8uUUZMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Replacing the missing values with mean value of each column\n",
        "df = df.fillna(df.select_dtypes(include='number').mean())\n"
      ],
      "metadata": {
        "id": "Zx9oEB86VkPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "6nwee4Y4V1r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "385iUcp4WFBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling Outliers"
      ],
      "metadata": {
        "id": "KC0019NjXtBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.boxplot(data=df,palette='rocket')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v4XGCIZkWZf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING IQR METHOD TO HANDLE OUTLIERS"
      ],
      "metadata": {
        "id": "tmRC7R8LYg38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numeric columns\n",
        "df_numeric = df.select_dtypes(include='number')\n",
        "\n",
        "# Calculate IQR for numeric data\n",
        "Q1 = df_numeric.quantile(0.25)\n",
        "Q3 = df_numeric.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Count of outliers in each numeric column\n",
        "outliers = ((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).sum()\n",
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "40PUDFlnYVgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outliers)\n"
      ],
      "metadata": {
        "id": "JRKiGQ64Yk23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_outlier = ['AH', 'C6H6(GT)', 'CO(GT)', 'NO2(GT)', 'NOx(GT)', 'PT08.S1(CO)',\n",
        "                  'PT08.S2(NMHC)', 'PT08.S3(NOx)', 'PT08.S4(NO2)', 'PT08.S5(O3)',\n",
        "                  'RH', 'T']\n",
        "\n",
        "# Convert columns to float\n",
        "for i in column_outlier:\n",
        "    df[i] = df[i].astype('float')\n",
        "\n",
        "# Calculate Q1, Q3, and IQR for only the relevant columns\n",
        "Q1 = df[column_outlier].quantile(0.25)\n",
        "Q3 = df[column_outlier].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Detect outliers\n",
        "outliers = (df[column_outlier] < (Q1 - 1.5 * IQR)) | (df[column_outlier] > (Q3 + 1.5 * IQR))\n",
        "\n",
        "# Replace outliers with median\n",
        "for i in column_outlier:\n",
        "    median_val = df[i].median()\n",
        "    df.loc[outliers[i], i] = median_val\n",
        "\n",
        "# Check if outliers remain\n",
        "remaining_outliers = ((df[column_outlier] < (Q1 - 1.5 * IQR)) |\n",
        "                      (df[column_outlier] > (Q3 + 1.5 * IQR))).sum()\n",
        "\n",
        "print(\"Remaining outliers after replacement:\")\n",
        "print(remaining_outliers)\n"
      ],
      "metadata": {
        "id": "he9INakxZCOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.boxplot(data=df,palette='rocket')\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.show()\n",
        "#we can see that the number of points outside the whiskers have reduced, indicating outliers have been handled"
      ],
      "metadata": {
        "id": "EMcw8dzmZdE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Select only numeric columns\n",
        "numeric_df = df.select_dtypes(include='number')\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(9, 5))\n",
        "sns.heatmap(numeric_df.corr(), cmap='YlGnBu', annot=True)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9yswNzGqZ87N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. CO(GT) and C6H6(GT) show strong positive correlations with several other features and pollutant indicators, especially gas sensor readings.\n",
        "\n",
        "2. NOx(GT) and NO2(GT) show moderate to strong correlations with related gas sensors, indicating their influence on air quality.\n",
        "\n",
        "3. Temperature (T), Relative Humidity (RH), and Absolute Humidity (AH) show low or negligible correlation with pollutant levels and sensor data, except for a moderate correlation between T and AH."
      ],
      "metadata": {
        "id": "Gb2kwP4zayuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore we use the pollutants with highest correlation as features. However, a clear range for C6H6 to calculate its AQI subindex could not be found and hence it has not been used as a feature."
      ],
      "metadata": {
        "id": "ibTlALdea94q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate subindex of CO\n",
        "def CO_AQI_subindex(x):\n",
        "    if x <= 1:\n",
        "        return x * 50 / 1\n",
        "    elif x <= 2:\n",
        "        return 50 + (x - 1) * 50\n",
        "    elif x <= 10:\n",
        "        return 100 + (x - 2) * 100 / 8\n",
        "    elif x <= 17:\n",
        "        return 200 + (x - 10) * 100 / 7\n",
        "    elif x <= 34:\n",
        "        return 300 + (x - 17) * 100 / 17\n",
        "    elif x > 34:\n",
        "        return 400 + (x - 34) * 100 / 17\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[\"CO_SubIndex\"] = df[\"CO(GT)\"].apply(lambda x: CO_AQI_subindex(x))"
      ],
      "metadata": {
        "id": "dW9aQ6BJaBzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##calculate subindex of NO2\n",
        "def NO2_AQI_subindex(x):\n",
        "    if x <= 40:\n",
        "        return x * 50 / 40\n",
        "    elif x <= 80:\n",
        "        return 50 + (x - 40) * 50 / 40\n",
        "    elif x <= 180:\n",
        "        return 100 + (x - 80) * 100 / 100\n",
        "    elif x <= 280:\n",
        "        return 200 + (x - 180) * 100 / 100\n",
        "    elif x <= 400:\n",
        "        return 300 + (x - 280) * 100 / 120\n",
        "    elif x > 400:\n",
        "        return 400 + (x - 400) * 100 / 120\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df[\"NO2_SubIndex\"] = df[\"NO2(GT)\"].apply(lambda x: NO2_AQI_subindex(x))"
      ],
      "metadata": {
        "id": "YUe87UbAbBAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##calculate subindex of NOx\n",
        "def NOx_AQI_subindex(x):\n",
        "    if x <= 40:\n",
        "        return x * 50 / 40\n",
        "    elif x <= 80:\n",
        "        return 50 + (x - 40) * 50 / 40\n",
        "    elif x <= 180:\n",
        "        return 100 + (x - 80) * 100 / 100\n",
        "    elif x <= 280:\n",
        "        return 200 + (x - 180) * 100 / 100\n",
        "    elif x <= 400:\n",
        "        return 300 + (x - 280) * 100 / 120\n",
        "    elif x > 400:\n",
        "        return 400 + (x - 400) * 100 / 120\n",
        "    else:\n",
        "        return 0\n",
        "df[\"NOx_SubIndex\"] = df[\"NOx(GT)\"].apply(lambda x: NOx_AQI_subindex(x))\n"
      ],
      "metadata": {
        "id": "KU5ERrt4bDX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "6TfocRZcbISI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating AQI\n",
        "df[\"AQI\"] = round(df[[\"NO2_SubIndex\", \"CO_SubIndex\", \"NOx_SubIndex\"]].max(axis = 1))"
      ],
      "metadata": {
        "id": "0BPF_cZObhF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "VMBaLBgVcVjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "y = df['Air Quality'].values\n",
        "features = ['CO(GT)', 'NO2(GT)', 'NOx(GT)']\n",
        "X = df[features].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n"
      ],
      "metadata": {
        "id": "n10qQRtycIP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a Naive bayes class\n",
        "class NaiveBayes:\n",
        "\n",
        "    #fitting the model\n",
        "    def fit(self, X, y):\n",
        "        samples, features = X.shape\n",
        "        self.classes = np.unique(y)\n",
        "        n_classes = len(self.classes)\n",
        "\n",
        "        # calculate mean, var, and prior for each class\n",
        "        self.mean = np.zeros((n_classes, features), dtype=np.float64)\n",
        "        self.var = np.zeros((n_classes, features), dtype=np.float64)\n",
        "        self.priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for index, c in enumerate(self.classes):\n",
        "            X_c = X[y == c]\n",
        "            self.mean[index, :] = X_c.mean(axis=0)\n",
        "            self.var[index, :] = X_c.var(axis=0)\n",
        "            self.priors[index] = X_c.shape[0] / float(samples)\n",
        "\n",
        "\n",
        "    def predict(self, X,noise_factor):\n",
        "        y_pred = [self._predict(x) for x in X]\n",
        "        return np.array(y_pred)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        probab = []\n",
        "\n",
        "        # calculate posterior probability for each class\n",
        "        for index, c in enumerate(self.classes):\n",
        "            prior = np.log(self.priors[index])\n",
        "            posterior = np.sum(np.log(self.pdf(index, x)))\n",
        "            posterior = posterior + prior\n",
        "            probab.append(posterior)\n",
        "\n",
        "        # introduce randomness by adding noise to the posteriors to see how to affects model performance\n",
        "        probab_with_noise = np.array(probab) + np.random.normal(scale=noise_factor, size=len(probab))\n",
        "\n",
        "        # return class with the highest posterior probability with noise added\n",
        "        return self.classes[np.argmax(probab_with_noise)]\n",
        "\n",
        "    #calculating proability distribution\n",
        "    def pdf(self, class_index, x):\n",
        "        mean = self.mean[class_index]\n",
        "        var = self.var[class_index]\n",
        "        num = np.exp(-((x - mean) ** 2) / (2 * var))\n",
        "        denom = np.sqrt(2 * np.pi * var)\n",
        "        return num / denom"
      ],
      "metadata": {
        "id": "vGHR9dSWc9Kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
        "    return accuracy\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "naive_bayes = NaiveBayes()\n",
        "naive_bayes.fit(X_train, y_train)\n",
        "#accuracy with noise included\n",
        "noise_factor=0.7\n",
        "prediction =naive_bayes.predict(X_test, noise_factor)\n",
        "print(\"Naive Bayes classification accuracy with noise\", accuracy(y_test, prediction))\n",
        "#accuracy without noise\n",
        "noise_factor=0\n",
        "y_pred =naive_bayes.predict(X_test, noise_factor)\n",
        "print(\"Naive Bayes classification accuracy without noise\", accuracy(y_test, y_pred))"
      ],
      "metadata": {
        "id": "76_qvNWgdwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "High accuracy (94.8%): Your Naive Bayes model does a great job at correctly predicting the air quality category using just 3 features.\n",
        "\n",
        "Small drop with noise (94.8% â 94.2%):\n",
        "\n",
        "Even after adding randomness (which simulates real-world uncertainty), the model's accuracy only dropped by 0.6%.\n",
        "\n",
        "That means your model is stable and robust, which is exactly what you want in real applications where data isn't always perfect.\n",
        "\n",
        "\n",
        "\n",
        "My model:\n",
        "\n",
        "Performs very well\n",
        "\n",
        "Is not overly sensitive to noise or uncertainty\n",
        "\n",
        "Is reliable for prediction tasks"
      ],
      "metadata": {
        "id": "W4BpSyvagEjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â Conclusion\n",
        "\n",
        "This implementation builds a Naive Bayes classifier from scratch to predict air quality categories based on pollutant levels (CO(GT), NO2(GT), and NOx(GT)). The classifier assumes a Gaussian (Normal) distribution for each feature within each class and uses these distributions to calculate class probabilities.\n",
        "\n",
        "To evaluate the model's robustness, controlled random noise is added to the posterior probabilities before making predictions. This simulates uncertainty in real-world scenarios and allows us to compare accuracy with and without noise.\n",
        "\n",
        "Results show how the model's performance can degrade when predictions are influenced by noise, providing insights into its stability and reliability under imperfect conditions."
      ],
      "metadata": {
        "id": "sdtXgHWcfW3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Air Quality Mapping according to AQI values\n",
        "def air_qual(x):\n",
        "    if 0 <= x <= 50:\n",
        "        return 1  # Good\n",
        "    elif x <= 100:\n",
        "        return 2  # Satisfactory\n",
        "    elif x <= 200:\n",
        "        return 3  # Moderately polluted\n",
        "    elif x <= 300:\n",
        "        return 4  # Poor\n",
        "    elif x <= 400:\n",
        "        return 5  # Very Poor\n",
        "    elif x > 400:\n",
        "        return 6  # Severe\n",
        "\n",
        "# Air quality class mapping\n",
        "class_map = {\n",
        "    1: \"Good\",\n",
        "    2: \"Satisfactory\",\n",
        "    3: \"Moderately polluted\",\n",
        "    4: \"Poor\",\n",
        "    5: \"Very Poor\",\n",
        "    6: \"Severe\"\n",
        "}\n",
        "\n",
        "# Example: Trained NaiveBayes model (make sure to run training code first!)\n",
        "# naive_bayes = NaiveBayes()\n",
        "# naive_bayes.fit(X_train, y_train)\n",
        "\n",
        "# Get user inputs for the pollutants\n",
        "co = float(input(\"Enter CO(GT) value (e.g., 2.5): \"))\n",
        "no2 = float(input(\"Enter NO2(GT) value (e.g., 35.0): \"))\n",
        "nox = float(input(\"Enter NOx(GT) value (e.g., 20.0): \"))\n",
        "\n",
        "# Prepare input data in the same format as used during training\n",
        "input_data = np.array([[co, no2, nox]])\n",
        "\n",
        "# Ask user if they want to include noise\n",
        "add_noise = input(\"Add noise to prediction? (yes/no): \").strip().lower()\n",
        "noise_factor = 0.7 if add_noise == \"yes\" else 0.0\n",
        "\n",
        "# Make prediction using Naive Bayes model\n",
        "prediction = naive_bayes.predict(input_data, noise_factor)\n",
        "\n",
        "# Map the predicted class to the air quality label\n",
        "predicted_class = prediction[0]\n",
        "predicted_air_quality = class_map[predicted_class]\n",
        "\n",
        "# Display the result\n",
        "print(f\"\\nPredicted Air Quality: {predicted_air_quality}\")\n"
      ],
      "metadata": {
        "id": "CQOivzGvd4BH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b614a4-e837-4248-873d-982902f79571"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter CO(GT) value (e.g., 2.5): 2.5\n",
            "Enter NO2(GT) value (e.g., 35.0): 35.0\n",
            "Enter NOx(GT) value (e.g., 20.0): 20.0\n",
            "Add noise to prediction? (yes/no): yes\n",
            "\n",
            "Predicted Air Quality: Moderately polluted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CW1b2FxgsyP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}